{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14480660,"sourceType":"datasetVersion","datasetId":9248920}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.saving import register_keras_serializable\nfrom tensorflow.keras import layers, models, Input, callbacks, regularizers\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\n\n#-----------------------\n#---configs so I don't have to search and change values at 10 different places in the script everytime I want to change smtg\n#---(lesson learnt the hard way)\n#-----------------------\nclass Config:\n    # --- Paths ---\n    H5_PATH = \"/kaggle/input/aspcapstar-dr17-150kstars/apogee_dr17_parallel.h5\" \n    TFREC_DIR = \"/kaggle/working/tfrecords\"\n    STATS_PATH = \"/kaggle/working/dataset_stats.npz\"\n    \n    # --- System ---\n    TESTING_MODE = False\n    TEST_LIMIT = 20000 \n    NUM_SHARDS = 16 \n    \n    # --- Model Hyperparameters ---\n    BATCH_SIZE = 512       \n    LEARNING_RATE = 1e-3  \n    EPOCHS = 100\n    LATENT_DIM = 268\n    OUTPUT_LENGTH = 8575\n    \n    # --loss related---\n    L2_VAL = 1e-4          \n    INPUT_NOISE = 0.05     \n    IVAR_SCALE = 1000.0   \n    CLIP_NORM = 1.0        \n\n    #----predictor-labels--------\n    SELECTED_LABELS = [\n        # 1. Core\n        'TEFF', 'LOGG', 'FE_H', 'VMICRO', 'VMACRO', 'VSINI',\n        # 2. CNO\n        'C_FE', 'N_FE', 'O_FE',\n        #3. metals\n        'MG_FE', 'SI_FE', 'CA_FE', 'TI_FE', 'S_FE',\n        'AL_FE', 'MN_FE', 'NI_FE', 'CR_FE' \n    ]\n\nconfig = Config()\nos.makedirs(config.TFREC_DIR, exist_ok=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:09:13.256947Z","iopub.execute_input":"2026-01-17T17:09:13.257151Z","iopub.status.idle":"2026-01-17T17:09:34.295791Z","shell.execute_reply.started":"2026-01-17T17:09:13.257128Z","shell.execute_reply":"2026-01-17T17:09:34.294967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_nans(h5_path, selected_labels):\n    \"\"\"\n    Scans the dataset and prints the percentage of unusable (NaN/Flagged) \n    data for each label.\n    \"\"\"\n    print(f\"Scanning {len(selected_labels)} labels for missing data...\")\n    print(f\"{'LABEL':<10} | {'MISSING %':<10} | {'STATUS'}\")\n    \n    missing_report = {}\n    \n    with h5py.File(h5_path, 'r') as f:\n        # Detect structure (Group vs Table)\n        if isinstance(f['metadata'], h5py.Group):\n            get_col = lambda k: f['metadata'][k][:]\n            keys = list(f['metadata'].keys())\n        else:\n            get_col = lambda k: f['metadata'][k]\n            keys = f['metadata'].dtype.names\n\n        # Get total count from the first label\n        total_stars = len(get_col(selected_labels[0]))\n        \n        for label in selected_labels:\n            # 1. Get Raw Values\n            raw_vals = get_col(label)\n            \n            # 2. Check Flags (The Robust Logic)\n            flag_name = f\"{label}_FLAG\"\n            if flag_name in keys:\n                flg = get_col(flag_name)\n                # Handle Void/Structured types\n                if flg.dtype.names: flg = flg[flg.dtype.names[0]]\n                if flg.dtype.kind == 'V': flg = flg.view('<i4')\n                \n                # Bad if Flag != 0\n                is_bad_flag = (flg.astype(int) != 0)\n            else:\n                is_bad_flag = np.zeros(total_stars, dtype=bool)\n                \n            # 3. Check Placeholder Values (Standard APOGEE -9999)\n            # We check < -100 to catch any weird negative placeholders\n            is_bad_val = (raw_vals < -100)\n            \n            # 4. Combine (Either Flagged OR Missing Value)\n            total_bad = np.logical_or(is_bad_flag, is_bad_val)\n            \n            # 5. Calculate Stats\n            bad_count = np.sum(total_bad)\n            pct = (bad_count / total_stars) * 100\n            missing_report[label] = pct\n            \n            # 6. Status Indicator\n            if pct < 5.0:\n                status = \"Great\"\n            elif pct < 20.0:\n                status = \"Okay\"\n            else:\n                status = \"Hell Nah\"\n            \n            print(f\"{label:<10} | {pct:>9.2f}% | {status}\")\n\n    print(\"-\" * 50)\n    return missing_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:09:34.297267Z","iopub.execute_input":"2026-01-17T17:09:34.297733Z","iopub.status.idle":"2026-01-17T17:09:34.305700Z","shell.execute_reply.started":"2026-01-17T17:09:34.297708Z","shell.execute_reply":"2026-01-17T17:09:34.305092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#==================================\n\ndef get_clean_imputed_data(h5_path, selected_labels, limit=None):\n    \n    print(\"Read data for imputation\")\n    \n    with h5py.File(h5_path, 'r') as f:\n        # Detect structure type\n        if isinstance(f['metadata'], h5py.Group):\n            get_col = lambda k: f['metadata'][k][:]\n            keys = list(f['metadata'].keys())\n        else:\n            get_col = lambda k: f['metadata'][k]\n            keys = f['metadata'].dtype.names\n\n        raw_values = np.stack([get_col(p) for p in selected_labels], axis=1)\n        bad_mask = np.zeros_like(raw_values, dtype=bool)\n        \n        for i, label in enumerate(selected_labels):\n            flag_name = f\"{label}_FLAG\"\n            if flag_name in keys:\n                flg = get_col(flag_name)\n                # FIX: Handle Void/Structured Types safely\n                if flg.dtype.names: flg = flg[flg.dtype.names[0]]\n                if flg.dtype.kind == 'V': flg = flg.view('<i4')\n                is_bad = (flg.astype(int) != 0)\n            elif label in ['TEFF', 'LOGG', 'VMICRO', 'VMACRO', 'VSINI']:\n                is_bad = (raw_values[:, i] < -5000)\n            else:\n                is_bad = np.zeros_like(raw_values[:, i], dtype=bool)\n            bad_mask[:, i] = is_ba\n\n    if limit:\n        print(f\"âš ï¸ Truncating to first {limit} stars.\")\n        raw_values = raw_values[:limit]\n        bad_mask = bad_mask[:limit]\n\n    print(f\"   ðŸš€ Imputing Labels for {len(raw_values)} stars...\")\n    vals_to_impute = raw_values.copy()\n    vals_to_impute[bad_mask] = np.nan\n    \n    imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, initial_strategy='median')\n    clean_labels = imputer.fit_transform(vals_to_impute)\n    return clean_labels\n\ndef _bytes_feature(value):\n    if isinstance(value, type(tf.constant(0))): value = value.numpy()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef generate_tfrecords():\n    \"\"\"Converts H5 to TFRecords (Runs once).\"\"\"\n    print(f\"ðŸš€ Starting TFRecord Generation (Shards: {config.NUM_SHARDS})...\")\n    limit = config.TEST_LIMIT if config.TESTING_MODE else None\n    \n    # 1. Clean Labels\n    clean_labels = get_clean_imputed_data(config.H5_PATH, config.SELECTED_LABELS, limit=limit)\n    \n    # 2. Save Stats\n    mean_labels = np.mean(clean_labels, axis=0)\n    std_labels = np.std(clean_labels, axis=0)\n    std_labels[std_labels == 0] = 1.0 \n    np.savez(config.STATS_PATH, mean=mean_labels, std=std_labels)\n    print(f\"âœ… Stats saved to {config.STATS_PATH}\")\n    \n    # 3. Write Shards\n    total_stars = len(clean_labels)\n    shard_size = int(np.ceil(total_stars / config.NUM_SHARDS))\n    \n    with h5py.File(config.H5_PATH, 'r') as f:\n        ds_flux = f['flux']\n        ds_ivar = f['ivar']\n        \n        for shard_id in range(config.NUM_SHARDS):\n            start_idx = shard_id * shard_size\n            end_idx = min((shard_id + 1) * shard_size, total_stars)\n            if start_idx >= total_stars: break\n            \n            filename = os.path.join(config.TFREC_DIR, f\"data_{shard_id:02d}.tfrec\")\n            print(f\"   Writing Shard {shard_id+1}/{config.NUM_SHARDS}: {filename}\")\n            \n            with tf.io.TFRecordWriter(filename) as writer:\n                chunk_flux = ds_flux[start_idx:end_idx]\n                chunk_ivar = ds_ivar[start_idx:end_idx]\n                chunk_labels = clean_labels[start_idx:end_idx]\n                \n                for i in range(len(chunk_labels)):\n                    label_bytes = tf.io.serialize_tensor(chunk_labels[i].astype(np.float32))\n                    spec_raw = np.stack([chunk_flux[i], chunk_ivar[i]], axis=-1).astype(np.float32)\n                    spec_bytes = tf.io.serialize_tensor(spec_raw)\n                    \n                    feature = {'labels': _bytes_feature(label_bytes), 'spectra': _bytes_feature(spec_bytes)}\n                    writer.write(tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString())\n    print(\"âœ… TFRecord Generation Complete.\")\n\n# Generate if missing\nif not glob.glob(os.path.join(config.TFREC_DIR, \"*.tfrec\")):\n    generate_tfrecords()\nelse:\n    print(\"âœ… TFRecords found. Skipping generation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:09:34.306442Z","iopub.execute_input":"2026-01-17T17:09:34.306657Z","iopub.status.idle":"2026-01-17T17:16:44.724161Z","shell.execute_reply.started":"2026-01-17T17:09:34.306638Z","shell.execute_reply":"2026-01-17T17:16:44.723442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Stats\nstats = np.load(config.STATS_PATH)\nMEAN_TENSOR = tf.constant(stats['mean'], dtype=tf.float32)\nSTD_TENSOR = tf.constant(stats['std'], dtype=tf.float32)\n\n# ==========================================\n \n# ==========================================\ndef parse_and_normalize(example_proto):\n    feature_desc = {\n        'labels': tf.io.FixedLenFeature([], tf.string),\n        'spectra': tf.io.FixedLenFeature([], tf.string),\n    }\n    parsed = tf.io.parse_single_example(example_proto, feature_desc)\n    labels = tf.io.parse_tensor(parsed['labels'], out_type=tf.float32)\n    spectra = tf.io.parse_tensor(parsed['spectra'], out_type=tf.float32)\n    \n    labels.set_shape([len(config.SELECTED_LABELS)])\n    spectra.set_shape([config.OUTPUT_LENGTH, 2])\n    \n    norm_labels = (labels - MEAN_TENSOR) / STD_TENSOR\n    return norm_labels, spectra\n\ndef build_dataset():\n    all_files = sorted(tf.io.gfile.glob(os.path.join(config.TFREC_DIR, \"*.tfrec\")))\n    split_idx = int(len(all_files) * 0.8)\n    if split_idx == len(all_files): split_idx -= 1\n    \n    train_files = all_files[:split_idx]\n    val_files = all_files[split_idx:]\n    print(f\"Data Split: {len(train_files)} Train Files, {len(val_files)} Val Files\")\n    \n    def load_files(filenames):\n        ds = tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.AUTOTUNE)\n        ds = ds.map(parse_and_normalize, num_parallel_calls=tf.data.AUTOTUNE)\n        return ds\n    \n    train_ds = load_files(train_files).shuffle(10000).batch(config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    val_ds = load_files(val_files).batch(config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return train_ds, val_ds\n\ntrain_ds, val_ds = build_dataset()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:16:44.725043Z","iopub.execute_input":"2026-01-17T17:16:44.725257Z","iopub.status.idle":"2026-01-17T17:16:44.845149Z","shell.execute_reply.started":"2026-01-17T17:16:44.725237Z","shell.execute_reply":"2026-01-17T17:16:44.844592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# Use a modiefied soblev loss function (I read in a paper it works well)\n# ==========================================\n\n@register_keras_serializable()\ndef sobolev_loss(y_true, y_pred):\n    real_flux = y_true[:, :, 0:1]\n    ivar = y_true[:, :, 1:2]\n    valid_mask = tf.cast(real_flux > -10.0, tf.float32)    \n    safe_flux = tf.where(valid_mask == 1.0, real_flux, y_pred)\n    ivar_safe = tf.clip_by_value(ivar / 1000.0, 0.0, 1.0)# scale and clip\n    weight=tf.where(((safe_flux<0.9) & (ivar>0)),tf.maximum(ivar_safe,tf.cast(1.0,dtype=tf.float32)),ivar_safe)\n    #chi2\n    mse_term = tf.square(safe_flux - y_pred) * weight * valid_mask\n    # calculate \"gradients\" (difference between adjacent pixels)\n    true_grad = safe_flux[:, 1:, :] - safe_flux[:, :-1, :]\n    pred_grad = y_pred[:, 1:, :] - y_pred[:, :-1, :]\n    \n    # Calculate Squared Error of gradients (sobolev loss term)\n    grad_sq_diff = tf.square(true_grad - pred_grad)\n    grad_mask = valid_mask[:, 1:, :] * valid_mask[:, :-1, :]\n    grad_trust = (weight[:, 1:, :] * weight[:, :-1, :])\n    # Apply mask to gradient loss\n    grad_loss = grad_sq_diff * grad_mask * grad_trust\n    \n    #pad last pixel\n    grad_loss = tf.pad(grad_loss, [[0,0], [0,1], [0,0]])\n    \n    # final loss\n    total_loss = (mse_term + (10.0 * grad_loss))\n    \n    #safety check\n    loss = tf.where(tf.math.is_finite(total_loss), total_loss, tf.zeros_like(total_loss))\n    \n    return tf.reduce_mean(loss)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:16:44.845924Z","iopub.execute_input":"2026-01-17T17:16:44.846174Z","iopub.status.idle":"2026-01-17T17:16:44.852965Z","shell.execute_reply.started":"2026-01-17T17:16:44.846148Z","shell.execute_reply":"2026-01-17T17:16:44.852330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# Array version so I plot the loss\n# ==========================================\n\ndef sobolev_loss_arr(y_true, y_pred):\n    real_flux = y_true[:, :, 0:1]\n    ivar = y_true[:, :, 1:2]\n    valid_mask = tf.cast(real_flux > -10.0, tf.float32)    \n    safe_flux = tf.where(valid_mask == 1.0, real_flux, y_pred)\n    ivar_safe = tf.clip_by_value(ivar / 1000.0, 0.0, 1.0)# scale and clip\n    weight=tf.where(((safe_flux<0.9) & (ivar>0)),tf.maximum(ivar_safe,tf.cast(1.0,dtype=tf.float32)),ivar_safe)\n    #chi2\n    mse_term = tf.square(safe_flux - y_pred) * weight * valid_mask\n    \n    # calculate \"gradients\" (difference between adjacent pixels)\n    true_grad = safe_flux[:, 1:, :] - safe_flux[:, :-1, :]\n    pred_grad = y_pred[:, 1:, :] - y_pred[:, :-1, :]\n    \n    # Calculate Squared Error of gradients (sobolev loss term)\n    grad_sq_diff = tf.square(true_grad - pred_grad)\n    grad_mask = valid_mask[:, 1:, :] * valid_mask[:, :-1, :]\n    grad_trust = (weight[:, 1:, :] * weight[:, :-1, :])\n    # Apply mask to gradient loss\n    grad_loss = grad_sq_diff * grad_mask * grad_trust\n    \n    #pad last pixel\n    grad_loss = tf.pad(grad_loss, [[0,0], [0,1], [0,0]])\n    \n    # final loss\n    total_loss = (mse_term + (10.0 * grad_loss))*ROI*ROI2*ROI3\n    \n    #safety check\n    loss = tf.where(tf.math.is_finite(total_loss), total_loss, tf.zeros_like(total_loss))\n    \n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:16:44.853818Z","iopub.execute_input":"2026-01-17T17:16:44.854175Z","iopub.status.idle":"2026-01-17T17:16:44.873577Z","shell.execute_reply.started":"2026-01-17T17:16:44.854145Z","shell.execute_reply":"2026-01-17T17:16:44.873068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scaled the sigmoid function so my model can actually predict 1.0 and 0.0\n# most probably  there are better ways to do this, but I don't know any\n\n@register_keras_serializable()\ndef scaled_sigmoid(x):\n    return 1.3 * tf.nn.sigmoid(x)-0.15\n\n\ndef build_model():\n    input_dim = len(config.SELECTED_LABELS)\n    inputs = Input(shape=(input_dim,))\n    #Adding guassian noise just in case my model fits too well (:clown face:)\n    #x = layers.GaussianNoise(config.INPUT_NOISE)(inputs)\n    x=inputs\n    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(config.L2_VAL))(x)\n    \n    #268*32 IS 8576 , very close to our output shape (8575)\n    x = layers.Dense(config.LATENT_DIM * 32, activation='relu',\\\n                     kernel_regularizer=regularizers.l2(config.L2_VAL))(x) # punish extreme weights\n    x = layers.Reshape((config.LATENT_DIM, 32))(x)\n    \n    #Progessively sharpen the image\n    filters = [64, 32, 32, 16, 16]\n    kernels = [7,  7,  5,  5,  3]\n\n    #residual blocks\n    for f, k in zip(filters, kernels):\n        x = layers.UpSampling1D(size=2)(x)\n        res = layers.Conv1D(f, 1, padding='same', kernel_regularizer=regularizers.l2(config.L2_VAL))(x)\n        x = layers.Conv1D(f, kernel_size=k, padding='same', kernel_regularizer=regularizers.l2(config.L2_VAL))(x)\n        x = layers.Activation('relu')(x)\n        x = layers.SpatialDropout1D(0.1)(x) #avoid overfitting\n        x = layers.Add()([x, res])\n\n    #final layer (crop the last pixel so output shape becomes 8575)\n    x = layers.Conv1D(1, kernel_size=3, padding='same', activation=scaled_sigmoid)(x)\n    outputs = layers.Cropping1D(cropping=(0, 1))(x)\n    \n    return models.Model(inputs, outputs, name=\"Spectrogram_Emulator\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:16:44.875418Z","iopub.execute_input":"2026-01-17T17:16:44.875865Z","iopub.status.idle":"2026-01-17T17:16:44.894834Z","shell.execute_reply.started":"2026-01-17T17:16:44.875842Z","shell.execute_reply":"2026-01-17T17:16:44.894252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-----------------------\n#     TRAIN THE MODEL\n#-----------------------\nmodel = build_model()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(config.LEARNING_RATE, clipnorm=config.CLIP_NORM), \n              loss=sobolev_loss)\n\ncallbacks_list = [\n    callbacks.ModelCheckpoint('best_emulator.keras', save_best_only=True, monitor='val_loss'),\n    callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n    callbacks.ReduceLROnPlateau(patience=1, factor=0.5, min_lr=1e-6, verbose=1)\n]\n\nprint(\"BEGIN TRAINING\")\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=config.EPOCHS,\n    callbacks=callbacks_list,\n    verbose=1\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:16:44.895841Z","iopub.execute_input":"2026-01-17T17:16:44.896186Z","iopub.status.idle":"2026-01-17T17:27:53.287438Z","shell.execute_reply.started":"2026-01-17T17:16:44.896165Z","shell.execute_reply":"2026-01-17T17:27:53.286436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('SPECTROGRAM_GENERATOR_2.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:31:11.053008Z","iopub.execute_input":"2026-01-17T17:31:11.053517Z","iopub.status.idle":"2026-01-17T17:31:11.214458Z","shell.execute_reply.started":"2026-01-17T17:31:11.053490Z","shell.execute_reply":"2026-01-17T17:31:11.213718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#----SAVE AND PREDICT-----\n\nimport matplotlib.pyplot as plt\n\nmodel=tf.keras.models.load_model('best_emulator.keras')\n\n\n\n\ndef plot_spectrum_comparison(y_true, y_pred,ivar=None,loss=None, labels=None, title=\"Stellar Reconstruction\"):\n    plt.style.use('dark_background') #dark mode >>>>>> \n    wavelengths=np.logspace(np.log10(1514),np.log10(1695),config.OUTPUT_LENGTH)\n    # APOGEE takes log wavelengths to space their pixels\n    fig, (ax1, ax2, ax3,ax4) = plt.subplots(\n        4, 1, figsize=(14, 20), \n        gridspec_kw={'height_ratios': [3,1,2,2]}, \n        sharex=True\n    )\n    \n    \n    #TOP= SPECTROGRAMS   \n    ax1.plot(wavelengths, y_true, color='red', alpha=0.8, linewidth=1, label='Ground Truth (APOGEE)')\n    ax1.plot(wavelengths, y_pred, color='cyan', alpha=0.9, linewidth=0.8, label='Emulator Prediction')\n    ax1.set_ylabel(\"Flux\", fontsize=12, color='white')\n    ax1.set_title(title, fontsize=16, fontweight='bold', color='white')\n    ax1.legend(frameon=False, fontsize=12)\n\n    \n    #Strong spectral lines that can be seen in stars collected from APOGEE\n    # AI generated, couldn't bother looking it up myself (I'll do it later)\n    lines = [\n        # --- Iron Peak & Core Elements ---\n        (1519, \"Fe\"),     # Strong Iron anchor\n        (1526, \"Mn\"),     # Manganese (Deep Line Driver)\n        (1539, \"Fe\"),     # Strong Iron\n        (1569, \"Fe/Ti\"),  # Strong blend\n        (1576, \"Mg\"),     # Magnesium Triplet Center (The \"Big One\")\n        (1605, \"Ni\"),     # Nickel (Deep Line Driver)\n        (1615, \"Ca/Fe\"),  # Calcium & Iron\n        (1667, \"Ni\"),     # Nickel\n        (1682, \"Ni\"),     # Nickel\n\n        # --- Alpha Elements ---\n        (1596, \"Si\"),     # Silicon (Very distinctive dip)\n        (1638, \"Si\"),     # Silicon\n        (1672, \"Al\"),     # Aluminum (The \"Deepest\" single line usually)\n\n        # --- Molecular Bandheads (The \"Continuum Shapers\") ---\n        (1533, \"CN\"),     # Cyanogen band\n        (1558, \"CO\"),     # Carbon Monoxide Bandhead (Start of 'sawtooth')\n        (1619, \"CO\"),     # CO Bandhead\n        (1661, \"CO\"), \n        (1675, \"Al\"),# CO Bandhead (Very strong in cool stars)\n    ]\n    \n    for wl, name in lines:\n        ax1.axvline(x=wl, color='magenta', alpha=0.3, linestyle='--')\n        ax1.text(wl, 1.15, name, color='magenta', rotation=90, fontsize=10)\n        \n    #BOTTOM=RESIDUALS\n    #compare how well the model fits the data\n    residuals = y_true - y_pred\n    ax2.plot(wavelengths, residuals, color='#FF5555', linewidth=0.8)\n    ax2.axhline(0, color='white', linestyle='--', alpha=0.5)\n    ax2.set_ylabel(\"Residuals\\n(True - Pred)\", fontsize=10)\n    \n    \n    for wl, name in lines:\n        ax2.axvline(x=wl, color='magenta', alpha=0.3, linestyle='--')\n        ax2.text(wl, 1.15, name, color='magenta', rotation=90, fontsize=10)\n    #ax2.set_ylim(-0.2, 0.2) => hopefully my model fits good enough for me to do this (it didn't ;-;)\n    \n    # show imp labels\n    if labels is not None:\n        info_str = f\"Teff: {labels[0]} K\\nLogg: {labels[1]:.2f}\\n[Fe/H]: {labels[2]:.2f}\"\n        ax1.text(0.02, 0.1, info_str, transform=ax1.transAxes, \n                 bbox=dict(facecolor='grey', alpha=0.8, edgecolor='gray'),\n                 fontsize=11, color='white')\n    ax3.plot(wavelengths,np.log(ivar)-10,color='red',linewidth=0.8)\n    ax3.set_ylabel(\"log(inverse variance)\",fontsize=10)\n    ax3.set_xlabel(\"Wavelength (in nm)\", fontsize=12)\n    s_gap=wavelengths[ivar>0]\n    ax3.scatter(s_gap,np.ones_like(s_gap),color='yellow')\n    if(loss!=None):\n        ax4.plot(wavelengths,loss,color='orange',linewidth=0.8)\n        ax4.set_ylabel(\"loss:\",fontsize=10)\n    fig.tight_layout() \n    fig.savefig(\"/kaggle/working/spectrogram_1.jpg\")\n    fig.show()\n\n#generate sample spectrograms\nfor batch_labels, batch_spectra in val_ds.take(1):\n   \n    preds = model.predict(batch_labels)\n\n    idx = 2\n    true_spec = batch_spectra[idx, :, 0] \n    ivar=batch_spectra[idx,:,1]\n    pred_spec = preds[idx, :, 0]\n    star_labels = batch_labels[idx]\n    star_labels=star_labels*STD_TENSOR+MEAN_TENSOR\n    loss=sobolev_loss_arr(batch_spectra[idx:(idx+1),:,:], preds[idx:(idx+1),:,:])\n    loss=loss[0,:,0]\n    plot_spectrum_comparison(true_spec, pred_spec,ivar,loss, labels=star_labels)\n    print(loss.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:29:32.794979Z","iopub.execute_input":"2026-01-17T17:29:32.795292Z","iopub.status.idle":"2026-01-17T17:29:35.245329Z","shell.execute_reply.started":"2026-01-17T17:29:32.795267Z","shell.execute_reply":"2026-01-17T17:29:35.244655Z"}},"outputs":[],"execution_count":null}]}