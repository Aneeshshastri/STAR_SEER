{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a39056df","cell_type":"code","source":"!apt-get update && apt-get install -y aria2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a23011a5-7499-4ed7-a5fd-e7eeb8641e76","cell_type":"code","source":"import os\nimport shutil\nimport subprocess\nimport requests\nimport numpy as np\nimport h5py\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# --- CONFIGURATION ---\nOUTPUT_DIR = \"/kaggle/working\"\nOUTPUT_FILENAME = \"apogee_dr17_parallel.h5\"\nOUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n\n# URLs\nCATALOG_URL = \"https://data.sdss.org/sas/dr17/apogee/spectro/aspcap/dr17/synspec_rev1/allStar-dr17-synspec_rev1.fits\"\nBASE_SAS_URL = \"https://data.sdss.org/sas/dr17/apogee/spectro/aspcap/dr17/synspec_rev1/\"\n\n# Filters\nMIN_SNR = 100 # only the best stars for me model\nMAX_STARS = 150000 \nMAX_WORKERS = 10  # 10 Parallel downloads, cuz I don't wanna wait till the end of time (plz don't ban me)\n\n\ndef get_star_url(star_row):\n    telescope = star_row['TELESCOPE']\n    field = star_row['FIELD']\n    apogee_id = star_row['APOGEE_ID']\n    fname = f\"aspcapStar-dr17-{apogee_id}.fits\"\n    url = f\"{BASE_SAS_URL}{telescope}/{field}/{fname}\"\n    return url, fname\n\ndef process_single_star(row, keep_cols, custom_dtype):\n    #download and extract\n    url, filename = get_star_url(row)\n    # Unique temp name to prevent threads overwriting each other\n    local_path = f\"/tmp/{filename}_{os.getpid()}_{np.random.randint(0,10000)}\"\n    \n    try:\n        # 1. Download\n        with requests.get(url, stream=True, timeout=15) as r:\n            if r.status_code != 200:\n                return None\n            with open(local_path, 'wb') as f:\n                shutil.copyfileobj(r.raw, f)\n\n        # 2. Extract Data\n        with fits.open(local_path) as hdul:\n            flux = hdul[1].data.astype('float32')\n            err  = hdul[2].data.astype('float32')\n            \n            # Create Inverse Variance (Masking bad pixels)\n            with np.errstate(divide='ignore'):\n                ivar = 1.0 / (err**2)\n            ivar[~np.isfinite(ivar)] = 0.0\n\n        # 3. Extract Labels\n        label_values = tuple(row[col] for col in keep_cols)\n        label_entry = np.array([label_values], dtype=custom_dtype)\n        \n        star_id = row['APOGEE_ID']\n        \n        return (flux, ivar, label_entry, star_id)\n\n    except Exception:\n        return None\n        \n    finally:\n        # 4. Clean up. 350 GB nahi he mere pas\n        if os.path.exists(local_path):\n            os.remove(local_path)\n\ndef download_and_pack():\n    print(\"--- DOWNLOADING CATALOG ---\")\n    catalog_path = \"/tmp/allStar.fits\"\n    \n    # clean previous incomplete files, if they exist\n    if os.path.exists(catalog_path):\n        if os.path.getsize(catalog_path) < 3 * 1024**3:\n            print(\"Deleting incomplete catalog...\")\n            os.remove(catalog_path)\n            \n    if not os.path.exists(catalog_path):\n        print(f\"Fetching catalog (using aria2c)...\")\n        cmd = f\"aria2c -x 16 -s 16 -q --file-allocation=none -d /tmp -o allStar.fits {CATALOG_URL}\"\n        #16 connections, safe (hopefully)\n        subprocess.run(cmd, shell=True, check=True)\n    \n    print(\"Reading Catalog Table...\")\n    catalog = Table.read(catalog_path)\n    \n    # --- PREPARE SCHEMA ---\n    keep_cols = []\n    dtype_list = []\n    for col in catalog.colnames:\n        if catalog[col].ndim == 1:\n            keep_cols.append(col)\n            dtype = catalog[col].dtype\n            if dtype.kind in ['U', 'S']:\n                dtype_list.append((col, 'S30')) \n            else:\n                dtype_list.append((col, dtype))\n    custom_dtype = np.dtype(dtype_list)\n\n    # --- FILTERING ---\n    print(\"Filtering stars...\")\n    mask = (catalog['SNR'] > MIN_SNR) & (catalog['ASPCAPFLAG'] == 0) & (catalog['TEFF'] > 0)\n    best_stars = catalog[mask]\n    \n    indices = np.arange(len(best_stars))\n    np.random.seed(42)\n    np.random.shuffle(indices)\n    limit = min(len(best_stars), MAX_STARS)\n    final_catalog = best_stars[indices[:limit]]\n    \n    print(f\"Targeting {len(final_catalog)} stars with {MAX_WORKERS} parallel threads.\")\n\n    with h5py.File(OUTPUT_PATH, 'w') as f:\n        # Initialize datasets\n        dset_flux = f.create_dataset(\"flux\", (0, 8575), maxshape=(None, 8575), dtype='float32', compression=\"gzip\")\n        dset_ivar = f.create_dataset(\"ivar\", (0, 8575), maxshape=(None, 8575), dtype='float32', compression=\"gzip\")\n        dset_labels = f.create_dataset(\"metadata\", (0,), maxshape=(None,), dtype=custom_dtype)\n        \n        # We use a ThreadPool to fetch files, but write them in the main thread (HDF5 is not thread-safe for writing)\n        #now we play the waiting game\n        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n            # We wrap 'process_single_star' to pass the constant arguments\n            future_to_star = {\n                executor.submit(process_single_star, row, keep_cols, custom_dtype): row \n                for row in final_catalog\n            }\n            \n            successful_count = 0\n            \n            # get result from threads\n            for future in tqdm(as_completed(future_to_star), total=len(final_catalog), desc=\"Parallel Download\"):\n                result = future.result()\n                \n                if result is not None:\n                    flux, ivar, label_entry, star_id = result\n                    \n                    # Sequential Write\n                    size = dset_flux.shape[0]\n                    dset_flux.resize(size + 1, axis=0)\n                    dset_ivar.resize(size + 1, axis=0)\n                    dset_labels.resize(size + 1, axis=0)\n                    \n                    dset_flux[size] = flux\n                    dset_ivar[size] = ivar\n                    dset_labels[size] = label_entry\n                    \n                    successful_count += 1\n    \n    print(f\"\\nSUCCESS! Extracted {successful_count} stars.\")\n    print(f\"Data saved to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    download_and_pack()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}